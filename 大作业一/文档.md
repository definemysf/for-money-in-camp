### 编程要求

在下列代码框中获取中国 500 强公司信息概况,具体要求如下：

1.获取所有页面中的公司名、法定代表人、注册时间以及证券类别，将获得到的内容保存到 csv 文件； 2.获取数据之后，分析 500 强公司的证券占比；

3.绘制饼图展示分析的结果。

部分数据展示如下：

```
新华人寿保险股份有限公司, 万峰, 1996-09-28, A股
中国民生银行股份有限公司, 洪崎, 1996-02-07, A股
兴业银行股份有限公司, 高建平, 1988-08-22, A股
上海浦东发展银行股份有限公司, 吉晓辉, 1992-10-19, A股
苏宁云商集团股份有限公司, 张近东, 2001-06-29, A股
中国太平保险集团有限责任公司, 王滨, 1982-02-13, 港股
华能国际电力股份有限公司, 曹培玺, 1994-06-30, A股
```

# 一、案例学习

## 中国500强公司信息爬取

### 项目背景

我国的高速发展离不开各式各样公司的共同努力，我国前 500 强公司可能大家都不太了解有那些，我们可以通过爬虫来获取前 500 强公司的概况。

### 任务描述

本实训是一个中国500强公司信息爬取的案例，主要是通过 `https://top.chinaz.com/gongsitop/index_500top.html` 该网站获取前 500 强公司的公司名、法定代表人、注册时间以及证券类别，将获得到的内容保存至 csv 文件。

### 明确爬取目标

首先，我们进入该网站：https://top.chinaz.com/gongsitop/index_500top.html ，可以看到前 500 强公司的大致信息。我们需要获取的是**公司名、法定代表人、注册时间以及证券类别**。

![img](D:\文件\all_typora_picture\WGxhNHJ3WE1mU3ZHSzljRVVjaldUdz09.png)

### 分析网页结构

既然我们已经了解了我们要爬取的内容，那接下来我们开始分析网页结构。首先需要了解我们需要的数据所在的位置，所以我们需要进入开发者模式（也可以鼠标右击，选择检查）查看数据的来源，这里我使用的是 Chrome 浏览器。我们可用通过全局搜索的方式来获取关键信息所在的位置。

![img](D:\文件\all_typora_picture\KzcwbFVaeTUraWFYcEI0YlFmbE1YQT09.png)

找到了数据所在的位置之后，我们可以发现数据就在当前页面链接的返回结果中，接下来我们观察下一页的 url 是怎么变化的。

![img](D:\文件\all_typora_picture\WHZJa1I3NUVmZW1GUmdxR1liWEtMZz09.png)

除了第一页是 index_500top 以外，其余页面的 url 都是 `index_500top` 加页数的形式。现在我们只需要获取到每一页的内容然后对其进行解析即可。

In [1]:

```
# 首先我们需要导入 requests 库
import requests
# 若是本地没有安装 requests 的同学可以通过“pip install requests”来安装
```

### requests爬取网页内容

这里我们首先需要对整个爬虫流程有一个了解：

**第一步**：对目标 url 进行请求，开发者工具中可以看到请求的具体信息，获取请求返回的内容。

![img](D:\文件\all_typora_picture\T215dDBHczlad01zdXorMlNUZkRuQT09.png)

从图中我们可以得到请求的 url 以及请求方式，该请求的方式是 GET 请求。

In [11]:

```
# 请求的url
url = "https://top.chinaz.com/gongsitop/index_500top.html"
# 设置请求头信息
headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36"
}
# 使用reqeusts模快发起 GET 请求
response = requests.get(url, headers=headers)
# 获取请求的返回结果
html = response.text
html
```

#### 使用正则表达式清洗网页信息

**第二步**：解析返回内容。从上面的返回结果可以看出返回的是 HTML 类型的数据，如果我们需要获取到关键信息，就需要使用到 XPATH 或者正则表达式等解析工具，这里我们使用的是 正则表达式。正则表达式是一种非常高性能的清洗方式，Python 中内置了 re 模块来实现正则表达式。我们需要导入这个库。

In [3]:

```
# 导入 re 模快
import re
```

在学习如何使用正则表达式进行文本查找/匹配之前，我们先简单的看一下 `Python` 中基本的正则表达式的语法。

| 字符 |                                                      意义 |
| ---: | --------------------------------------------------------: |
|    . |                            匹配除换行符之外的任何单个字符 |
|    * |                             匹配前面的子表达式 0 次到多次 |
|    + |                             匹配前面的子表达式 1 次到多次 |
|    ? |                             匹配前面的子表达式不超过 1 次 |
|   \| |                        或操作，匹配前面或者后面的子表达式 |
|    - |                            范围操作符，如 `0-9`、`a-z` 等 |
|    ^ |                                                  匹配行首 |
|    $ |                                                  匹配行尾 |
|  ( ) |                                    小括号用以分组子表达式 |
|  { } |                          花括号指定前面子表达式重复的次数 |
|  [ ] |                  中括号引导字母池，匹配其中的任何一个符号 |
|  [^] | 中括号以 ^ 开头，为逆向字母池，匹配任何一个不在其中的符号 |

此外，还有一些特殊的转义字符，比如：

| 转义符 |                                            匹配项目 |
| -----: | --------------------------------------------------: |
|      . |                                            匹配 `.` |
|      \ |                                            匹配 `\` |
|     \r |                                          匹配换行符 |
|     \n |                                          匹配回车符 |
|     \b |                                          单词分界符 |
|     \B |                                        非单词分界符 |
|     \s |               匹配任一空白字符，等价于 [\f\n\r\t\v] |
|     \S |                匹配非空白字符，等价于 [^\f\n\r\t\v] |
|     \w | 匹配任一英文字母、数字、下划线，等价于 [a-zA-Z_0-9] |
|     \W |                                等价于 [^a-zA-Z_0-9] |
|     \d |                                        等价于 [0-9] |
|     \D |                                       等价于 [^0-9] |

##### 常用函数

与正则表达式相关的常用函数有：

- `compile()`：根据正则表达式生成 re 对象；
- `match()`：尝试从字符串首部匹配某正则表达式，若成功则返回一个 match 对象，否则返回 None；
- `search()`：尝试搜算字符串中匹配某正则表达式的部分，若成功则返回一个 match 对象，否则返回 None；
- `fullmatch()`：尝试匹配整个字符串，若成功则返回一个 match 对象，否则返回 None；
- `findall()`：返回由指定字符串中所有匹配该模式的字串组成的列表；
- `sub()`: 用指定字符串或者替换法则（需要一个函数）来替换目标字符串中所有匹配该表达式的子串。

一个`match`对象主要包含下面的要素：

- `group`，其中 `group(0)` 包含匹配的整个模式内容，`group(1)`为第一个子模式，`group(2)` 为第二个子模式，以此类推 ……
- `span()`： 表明了匹配的区间，由两个分量 `begin()` 和 `end()` 组成。

观察数据在 html 中的位置，然后编写正则表达式解析。

![img](D:\文件\all_typora_picture\eGpzUm43eHZhQlVIUVVzOFgxbDkxQT09.png)

In [4]:

```python
# 使用 findall 函数来获取数据
# 公司名
company = re.findall('<a.*?target="_blank">(.+?)</a></h3>', html)
# 法定代表人
person = re.findall('法定代表人：</span>(.+?)</p>', html)
# 注册时间
signDate = re.findall('注册时间：</span>(.+?)</p>', html)
# 证券类别
category = re.findall('证券类别：</span>(.+?)</p>', html)
category
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-4-c50e2fa6c932> in <module>
      1 # 使用 findall 函数来获取数据
      2 # 公司名
----> 3 company = re.findall('<a.*?target="_blank">(.+?)</a></h3>', html)
      4 # 法定代表人
      5 person = re.findall('法定代表人：</span>(.+?)</p>', html)

NameError: name 're' is not defined
```

获取到指定的数据之后，我们可以使用 zip 含数将数据打包并转换成列表。

In [3]:

```python
pageOne = list(zip(company, person, signDate, category))
pageOne
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-3-bbe692ea52e0> in <module>
----> 1 pageOne = list(zip(company, person, signDate, category))
      2 pageOne

NameError: name 'company' is not defined
```

获取到第一页的数据之后，我们可以使用 for 循环获取其他页面的所有数据。

In [5]:

```python
# 存储内容
message = []
# 总共16个页面的数据
for page in range(16):
    # 组装url
    if page == 0:
        url = "https://top.chinaz.com/gongsitop/index_500top.html"
    else:
        url = "https://top.chinaz.com/gongsitop/index_500top_{}.html".format(page + 1)
    # 使用reqeusts模快发起 GET 请求
    response = requests.get(url, headers=headers)
    html = response.text
    # 使用 findall 函数来获取数据
    # 公司名
    company = re.findall('<a.*?target="_blank">(.+?)</a></h3>', html)
    # 法定代表人
    person = re.findall('法定代表人：</span>(.*?)</p>', html)
    # 注册时间
    signDate = re.findall('注册时间：</span>(.*?)</p>', html)
    # 证券类别
    category = re.findall('证券类别：</span>(.*?)</p>', html)
    pageOne = list(zip(company, person, signDate, category))
    # 合并列表
    message.extend(pageOne)
message

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-5-83ac35ec9e02> in <module>
      9         url = "https://top.chinaz.com/gongsitop/index_500top_{}.html".format(page + 1)
     10     # 使用reqeusts模快发起 GET 请求
---> 11     response = requests.get(url, headers=headers)
     12     html = response.text
     13     # 使用 findall 函数来获取数据

NameError: name 'requests' is not defined
```

**第三步**：保存内容。我们可以将 message 中的数据保存到数据库或者文件中，这里我们选择保存到 csv 文件。

In [17]:

```python
# 导入python中的内置模块csv
import csv
with open("content.csv", "w") as f:
    w = csv.writer(f)
    w.writerows(message)
```

In [18]:

```python
!cat content.csv
```

### 数据可视化

获取到数据之后，如果我们想要更加直观的看到我们的数据，可以对数据进行可视化操作。Python 的第三方库 matplotlib 是对数据进行可视化很好的一个选择，接下来我们通过绘制饼图来查看 500 强公司各证券类型的占比。

In [20]:

```python
import pandas as pd

# 读取数据
df = pd.read_csv("content.csv", names=["company", "person", "signDate", "category"])
df.head()
```

数据集为我们刚刚使用爬虫获取的数据，company 列为公司名，person 列为法定代表人，signDate 列为注册日期，category 列为证券类型。我们可以通过 DataFrame 的 info 方法查看数据的基本情况。

In [23]:

```python
df.info()
```

In [24]:

```python
# 根据证券类型进行分组
df1 = df.groupby("category").count()["company"]
df1
```

In [25]:

```python
# 在jupyter中直接展示图像
%matplotlib inline
import matplotlib.pyplot as plt

# 用黑体显示中文
plt.rcParams['font.sans-serif'] = ['SimHei']  

# 每个扇形的标签
labels = df1.index
# 每个扇形的占比
sizes = df1.values

fig1, ax1 = plt.subplots()
# 绘制饼图
ax1.pie(sizes, labels=labels, autopct='%d%%',radius=2,textprops={'fontsize': 20},
        shadow=False, startangle=90)
ax1.axis()

plt.show()
```

### 课后练习

1.获取[注册资金500强公司](https://top.chinaz.com/gongsi/index_zhuce.html)的名字和注册资金，并通过 matplotlib 绘制出注册资金最多的公司 top20 。

In [1]:

```python
#### 代码窗口
import requests
# 请求的url
url = "https://top.chinaz.com/gongsi/index_zhuce.html"
# 设置请求头信息
headers = {
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0"
}
# 使用reqeusts模快发起 GET 请求
response = requests.get(url, headers=headers)
# 获取请求的返回结果
html = response.text
html
import re
# 使用 findall 函数来获取数据
# 公司名
company = re.findall('<a.*?target="_blank">(.+?)</a></h3>', html)
# 注册资金
signMoney = re.findall('注册资本：</span>(.+?)</p>', html)
signMoney
pageOne = list(zip(company, signMoney))
pageOne
# 存储内容
message = []
# 总共16个页面的数据
for page in range(16):
    # 组装url
    if page == 0:
        url = "https://top.chinaz.com/gongsi/index_zhuce.html"
    else:
        url = "https://top.chinaz.com/gongsi/index_zhuce_{}.html".format(page + 1)
    # 使用reqeusts模快发起 GET 请求
    response = requests.get(url, headers=headers)
    html = response.text
    # 使用 findall 函数来获取数据
    # 公司名
    company = re.findall('<a.*?target="_blank">(.+?)</a></h3>', html)
    # 注册时间
    signMoney = re.findall('注册资本：</span>(.*?)</p>', html)
    pageOne = list(zip(company, signMoney))
    # 合并列表
    message.extend(pageOne)
message
# 导入python中的内置模块csv
import csv
with open("content.csv", "w") as f:
    w = csv.writer(f)
    w.writerows(message)
!cat content.csv
import pandas as pd
# 读取数据
df = pd.read_csv("content.csv", names=["company", "signMoney"])
df.head()
df.info()
# 根据证券类型进行分组
df1 = df.sorted("signMoney").count()["company"]
df1
# 在jupyter中直接展示图像
%matplotlib inline
import matplotlib.pyplot as plt

# 用黑体显示中文
plt.rcParams['font.sans-serif'] = ['SimHei']  



# 将数据写入CSV文件
with open("content.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["company", "signMoney"])  # 写入表头
    writer.writerows(message)  # 写入数据

# 读取CSV文件并处理数据
df = pd.read_csv("content.csv")

# 将注册资金转换为数值类型（可能需要根据实际数据格式调整）
df['signMoney'] = df['signMoney'].str.replace('万元', '').str.replace('亿', '0000').astype(float)

# 按注册资金降序排序并提取前20名
df_top20 = df.sort_values(by='signMoney', ascending=False).head(20)

# 提取公司名称和注册资金
company_names = df_top20['company'].tolist()
sign_money = df_top20['signMoney'].tolist()

# 绘制柱状图
plt.figure(figsize=(12, 8))  # 设置图形大小
plt.bar(company_names, sign_money, color='skyblue')  # 绘制柱状图

# 添加标题和标签
plt.title("Top 20 Companies by Registered Capital", fontsize=14)
plt.xlabel("Company", fontsize=12)
plt.ylabel("Registered Capital (in ten thousand)", fontsize=12)

# 旋转X轴标签，避免重叠
plt.xticks(rotation=45, ha='right')

# 显示图形
plt.tight_layout()  # 自动调整布局
plt.show()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-1-bad4ff36ef00> in <module>
      8 }
      9 # 使用reqeusts模快发起 GET 请求
---> 10 response = requests.get(url, headers=headers)
     11 # 获取请求的返回结果
     12 html = response.text

/usr/local/lib/python3.6/site-packages/requests/api.py in get(url, params, **kwargs)
     73     """
     74 
---> 75     return request('get', url, params=params, **kwargs)
     76 
     77 

/usr/local/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---> 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

/usr/local/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    513             hooks=hooks,
    514         )
--> 515         prep = self.prepare_request(req)
    516 
    517         proxies = proxies or {}

/usr/local/lib/python3.6/site-packages/requests/sessions.py in prepare_request(self, request)
    451             auth=merge_setting(auth, self.auth),
    452             cookies=merged_cookies,
--> 453             hooks=merge_hooks(request.hooks, self.hooks),
    454         )
    455         return p

/usr/local/lib/python3.6/site-packages/requests/models.py in prepare(self, method, url, headers, files, data, params, auth, cookies, hooks, json)
    317         self.prepare_method(method)
    318         self.prepare_url(url, params)
--> 319         self.prepare_headers(headers)
    320         self.prepare_cookies(cookies)
    321         self.prepare_body(data, files, json)

/usr/local/lib/python3.6/site-packages/requests/models.py in prepare_headers(self, headers)
    449         self.headers = CaseInsensitiveDict()
    450         if headers:
--> 451             for header in headers.items():
    452                 # Raise exception on invalid header value.
    453                 check_header_validity(header)

AttributeError: 'set' object has no attribute 'items'
```